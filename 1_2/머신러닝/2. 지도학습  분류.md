# 2. 지도학습 : 분류

---



## 분류의 개념

- 입력 데이터를 이미 정의된 클래스로 구분

### 경정경계 g(x;0)을 얻는 두 가지 접근법

	#### 확률기반방법

- 조건부확률

- 베이즈 분류기



#### 데이터 기반방법

- 데이터 간의 관계를 바탕으로 분류

- K-최근접이웃 분류기

  

## 베이즈 분류기

### 분류절차

1. 데이터 수집 

2. 클래스별 분포함수 추정

3. 테스트 데이터(X new) 입력 

4. 각 클래스별 판별함수 값 계산 

   gk(X new) = p(x|Ck)p(Ck) 

5. gk(X new)가 가장 큰 클래스 k로 할당



### 베이즈 분류기 구현

- 가우시안 확률분포를 가정한 베이즈 분류기
  - 가우시안 분포의 확률밀도 함수 : **평균과 공분산**행렬을 각각 추정
  - ![image-20220820151521599](C:\Users\82105\Desktop\방통대\1_2\머신러닝\IMAGE\mach.png)

- 간소화 방법

  - 공분산 행렬 시그마가 모두 단위행렬 -> 최소거리 분류기

   

- 모든클래스의 공분산이 동일하게 단위행렬의 상수배인 행렬을 가지는 경우 - 하나의 시그마만 추정
  - 마할라노비스 거리



경정 규칙 : y(x) = argmin(i) {(x-거리i)T{X-거리i}}

최소거리 분류기

- 정규화된 유클리디안 거리 : 요소별로 표준편차 값으로 나우어 준 후 유클리디안 거리 계산



### 베이즈 정리를 이용한 경정경계

- 이진분류 -> 데이터 X가 주어졌을 때 C1에 속하는지 C2에 속하는지
- 우도비 : 각 클래스에서 X가 관찰될 확률밀도의 비율
- **베이즈분류기** : 배이즈 정리로부터 유도된 결정경계를 이용한 분류



## K-최근접이웃 분류기

- 클래스 상관없이 모든 데이터 중 가장 작은 거리값을 갖는 데이터의 클래스로 할당



## 가우시안 베이즈 분류기 vs K-최근접 이웃 분류기

### 가우시안 베이즈 분류기

- 각 클래스에 대한 확률분포함수를 미리 가정하고 추정
- 학습 데이터를 통해 평균과 표준편차만 계산하여 활용
  - 분류 과정에서 학습데이터 불필요



### K-최근접 이웃분류기

- 비선형 형태를 갖기 때문에 복잡한 데이터에도 어느정도 분류성능 제공
- 확률분포모델을 미리 가정하지 않고 데이터 집합을 이용해 추정

- 새 데이터가 주어질 때마다 학습데이터 전체와의 거리계산 필요
  - 항상 학습데이터 저장 -> 비용(계산량, 메모리) 증가